descricao: Front-Matter of the proceedings of BRACIS 2023.

descricao: Data collection in many engineering fields involves multivariate time series gathered from a sensor network. These sensors often display differing sampling rates, missing data, and various irregularities. To manage these issues, complex preprocessing mechanisms are required, which become coupled with any statistical model trained with the transformed data. Modeling the motion of seabed-anchored floating platforms from measurements is a typical example for that. We propose and analyze a model that uses both recurrent and graph neural networks to handle irregularly sampled multivariate time series, while maintaining low computational cost. In this model, each time series is represented as a node in a heterogeneous graph, where edges depict the relationships between each measured variable. The time series are encoded using independent recurrent neural networks. A graph neural network then propagates information across the time series using attention layers. The outcome is a set of updated hidden representations used by the recurrent neural networks to create forecasts in an autoregressive manner. This model can generate forecasts for all input time series simultaneously while remaining lightweight. We argue that this architecture opens up new possibilities as the model can be integrated into low-capacity systems without needing expensive GPU clusters for inference.

descricao: Context: The use of Artificial Intelligence (AI) in various sectors of the economy is already a reality in Brazil. Consequently, since 2019, the number of cases in the Judiciary involving AI has increased. Cases involving facial recognition systems (FRS) for contracting bank credit are increasing annually, so it is necessary to analyze how the Judiciary handles the issues. Problem: Why is the São Paulo Court of Appeal ruling in favor of banks in all cases involving taking out credit through facial recognition technology? Methodology and Methods: Data were collected and processed using automated computer programs. The qualitative analysis used the analytical, comparative and monographic methods. Results: The Court of Appeal of São Paulo considers it difficult to deceive an AI system, therefore, the burden of proof is on the author, even if there is a consumer relationship. That is, the decisions are contrary to the general rule of the Code of Consumer Protection in Brazil, which consists of reversing the burden of proof in consumer relations when one of the parties is underprivileged. Contributions and Solutions: The research points to the path of jurisprudence in cases involving the contracting of credit through FRS, and the Judiciary is deciding against the bank’s customers, dispensing with the production of evidence by the banking sector. Therefore, it is necessary to alert the National Council of Justice and the Central Bank regarding this situation so that it is disciplined adequately since the FRS is fallible and does not guarantee the absence of fraud.

descricao: The development of tools based on robust mathematical models to deal with real-world, computationally intractable problems has increasingly aligned with combinatorial optimization and integer linear programming techniques due to enormous technological advances and the real need to deal with large volumes of data. In this paper, the Maximum Tropical Path Problem on graphs (MTPP), which is known to be NP-hard, was investigated. It is a problem of searching for specific structural patterns in networks that can represent various applications, among them biological interactions, such as metabolic, neurological or protein interaction networks. The main result of this work consists of a polynomial-time heuristic algorithm that, according to experimental results, finds good solutions in practice. Furthermore, an integer linear programming model was developed and implemented so that it could be compared with the heuristic presented. To conclude, an empirical analysis was performed through computational experiments on both random and real-word instances to evaluate the results presented in this paper.

descricao: Unsupervised and semi-supervised machine learning is very advantageous in data-intensive applications. Density-based hierarchical clustering obtains a detailed description of the structures of clusters and outliers in a dataset through density functions. The resulting hierarchy of these algorithms can be derived from a minimal spanning tree whose edges quantify the maximum density required for the connected data to characterize clusters, given a minimum number of objects, MinPts, in a given neighborhood. CORE-SG is a powerful spanning graph capable of deriving multiple hierarchical solutions with different densities with computational performance far superior to its predecessors. However, density-based algorithms use pairwise similarity calculations, which leads such algorithms to an asymptotic complexity of O(n2) for n objects in the dataset, impractical in scenarios with large amounts of data. This article enables hierarchical machine learning models based on density by reducing the computational cost with the help of Data Bubbles, focusing on clustering and outlier detection. It presents a study of the impact of data summarization on the quality of unsupervised models with multiple densities and the gain in computational performance. We provide scalability for several machine learning methods based on these models to handle large volumes of data without a significant loss in the resulting quality, enabling potential new applications like density-based data stream clustering.

descricao: In recent years, there has been considerable growth in the volume of legal proceedings in Brazil. In this context, there is a lot of potential in using recent advances in Natural Language Processing to automate tasks and analysis in the legal domain. In this article, we investigate text decoding methods for automating the writing of keyphrases, a sequence of key terms present in documents used in courts throughout Brazil. For this purpose, a text-to-text framework based on generative Transformers is used to generate keyphrases and evaluate three decoding techniques: greedy, top-K, and top-p. Since the keyphrases are designed to improve retrieval tasks, we evaluated keyphrases generated by the decoding methods in legal document retrieval. Traditional retrieval methods (TF-IDF and BM25) were used to evaluate the quality of the generated keyphrases. The results obtained (in terms of IR metrics) were statistically significant, and they indicate that greedy decoding generates high-quality keyphrases for the dockets used in this work, providing keyphrases close to the ones generated by human specialists.

descricao: Exploring label correlations is one of the main challenges in multi-label classification. The literature shows that prediction performances can be improved when classifiers learn these correlations. On the other hand, some works also argue that the multi-label classification methods cannot explore label correlations. The traditional multi-label local approach uses only information from individual labels, which makes it impractical to find relationships between them. In contrast, the multi-label global approach uses information from all labels simultaneously and may miss more specific relationships that are relevant. To overcome these limitations and verify if improving the prediction performances of multi-label classifiers is possible, we propose using Community Detection Methods to model label correlations and partition the label space into partitions between the local and global ones. These partitions, here named hybrid partitions, are formed of disjoint clusters of correlated labels, which are then used to build multi-label datasets and train multi-label classifiers. Since our proposal can generate several hybrid partitions, we validate all of them and choose the one that is considered the best. We compared our hybrid partitions with the local and global approaches and an approach that generates random partitions. Although our proposal improved the predictive performance of the used classifier in some datasets compared with other partitions, it also showed that, in general, independent of the approach used, the classifier still has difficulties learning several labels and predicting them correctly.

descricao: General Game Playing (GGP) is a challenging domain for AI agents, as it requires them to play diverse games without prior knowledge. In this paper, we develop a strategy to improve move suggestions in time-constrained GGP settings. This strategy consists of a hybrid version of UCT that combines Sequential Halving and UCB, favoring information acquisition in the root node, rather than overspend time on the most rewarding actions. Empirical evaluation using a GGP competition scheme from the Ludii framework shows that our strategy improves the average payoff over the entire competition set of games. Moreover, our agent makes better use of extended time budgets, when available.

descricao: In Stochastic Shortest Path (SSP) problems, not always the requirement of having at least one policy with a probability of reaching goals (probability-to-goal) equal to 1 can be met. This is the case when dead ends, states from which the probability-to-goal is equal to 0, are unavoidable for any policy, which demands the definition of alternate methods to handle such cases. The α-strong probability-to-goal priority is a property that is maintained by a criterion if a necessary condition to optimality is that the ratio between the probability-to-goal values of the optimal policy and any other policy is bound by a value of 0 ≤ α ≤ 1. This definition is helpful when evaluating the preference of different criteria for SSPs with dead ends. The Min-Cost given Max-Prob (MCMP) criterion is a method that prefers policies that minimize a well-defined cost function in the presence of unavoidable dead ends given policies that maximize probability-to-goal. However, it only guarantees α-strong priority for α = 1. In this paper, we define α-MCMP, a criterion based on MCMP with the addition of the guarantee of α-strong priority for any value 0 ≤ α ≤ 1. We also perform experiments comparing α-MCMP and GUBS, the only other criteria known to have α-strong priority for 0 ≤ α ≤ 1, to analyze the difference between the probability-to-goal of policies generated by each criterion.

descricao: Automated Planning is the subarea of AI devoted to developing algorithms that can solve sequential decision making problems. By taking a formal description of the environment, a planning algorithm generates a plan of actions (also called policy) that can guide an agent to accomplish a certain task. Classical planning assumes the environment is fully-observed and evolves in a deterministic way considering only simple reachability goals (e.g. a set of states to be reached by a plan or policy). In this work, we approach fully-observed non-deterministic planning (FOND) tasks which allow the specification of complex goals such as the preference over policy quality (weak, strong or strong-cyclic) and preferences over states in the paths generated by a policy. To solve this problem we propose formulae in α-CTL (branching time) temporal logic and use planning as model checking algorithms based on α-CTL to generate a solution that captures both, agent’s preferences and the desired policy quality. To evaluate the effectiveness of the proposed formulae and algorithms, we run experiments in the Rovers benchmark domain. Up to our knowledge, this is the first work to solve non-deterministic planning problems with preferences using a CTL temporal logic.

descricao: Support Vector Classifier (SVC) is a well-known Machine Learning (ML) model for linear classification problems. It can be used in conjunction with a reject option strategy to reject instances that are hard to correctly classify and delegate them to a specialist. This further increases the confidence of the model. Given this, obtaining an explanation of the cause of rejection is important to not blindly trust the obtained results. While most of the related work has developed means to give such explanations for machine learning models, to the best of our knowledge none have done so for when reject option is present. We propose a logic-based approach with formal guarantees on the correctness and minimality of explanations for linear SVCs with reject option. We evaluate our approach by comparing it to Anchors, which is a heuristic algorithm for generating explanations. Obtained results show that our proposed method gives shorter explanations with reduced time cost. Furthermore, although our approach is demonstrated with linear SVCs, it can be easily adapted to other classifiers with reject option, such as neural networks and random forests.

descricao: Alongside the increased use of algorithms as decision making tools, there have been an increase of cases where minority classes have been harmed. This gives rise to study of algorithmic fairness that deals with how to include fairness aspects in the design of algorithms. With this in mind, we define a new problem of fair coverage called Multi-Attribute Fairer Cover, that deals with the task of selecting a subset for training that is as fair as possible. We applied our method to the age regression model using instances from the UTKFace dataset. We also present computational experiments for an Integer Linear Programming model and for the age regression model. The experiments showed significant reduction on the error of the regression model when compared to a random selection.

descricao: Protein structure allows for an understanding of its function and enables the evaluation of possible interactions with other proteins. The molecular distance geometry problem (MDGP) regards determining a molecule’s three-dimensional (3D) structure based on the known distances between some pairs of atoms. An important application consists in finding 3D protein arrangements through data obtained by nuclear magnetic resonance (NMR). This work presents a study concerning the discretized version of the MDGP and the viability of employing genetic algorithms (GAs) to look for optimal solutions. We present computational results for input instances whose sizes varied from 10 to 103 atoms. The results obtained show that approaches to solving the discrete version of the MDGP based on GAs are promising.

descricao: We consider a generalization of the task allocation problem. A finite number of human resources are dynamically available to try to accomplish tasks. For each assigned task, the resource can fail or complete it correctly. Each task must be completed a number of times, and each resource is available for an independent number of tasks. Resources, tasks, and the probability of a correct response are modeled using Item Response Theory. The task parameters are known, while the ability of the resources must be learned through the interaction between resources and tasks. We formalize such a problem and propose an algorithm combining shadow test replanning to plan under uncertain knowledge, aiming to allocate resources optimally to tasks while maximizing the number of completed tasks. In our simulations, we consider three scenarios that depend on knowledge of the ability of the resources to solve the tasks. Results are presented using real data from the Mathematics and its Technologies test of the Brazilian Baccalaureate Examination (ENEM).

descricao: A new multi-algorithm approach for the daily optimization of thermal power plants and the Economic Dispatch was tested. For this, the dispatches were clustered in different groups based on their duration and the total power output requested. Genetic Algorithm, Differential Evolution and Simulated Annealing were selected for implementation and were employed according to the distinct characteristics of each dispatch. A monthly improvement of up to 2,45 x 105 R$ in the gross profit of the thermal power plant with the use of the optimization tool was estimated.

descricao: The increasing advancements in the field of machine learning have led to the development of numerous applications that effectively address a wide range of problems with accurate predictions. However, in certain cases, accuracy alone may not be sufficient. Many real-world problems also demand explanations and interpretability behind the predictions. One of the most popular interpretable models that are classification rules. This work aims to propose an incremental model for learning interpretable and balanced rules based on MaxSAT, called IMLIB. This new model was based on two other approaches, one based on SAT and the other on MaxSAT. The one based on SAT limits the size of each generated rule, making it possible to balance them. We suggest that such a set of rules seem more natural to be understood compared to a mixture of large and small rules. The approach based on MaxSAT, called IMLI, presents a technique to increase performance that involves learning a set of rules by incrementally applying the model in a dataset. Finally, IMLIB and IMLI are compared using diverse databases. IMLIB obtained results comparable to IMLI in terms of accuracy, generating more balanced rules with smaller sizes.

descricao: The discrete Choquet Integral (CI) and its generalizations have been successfully applied in many different fields, with particularly good results when considered in Fuzzy Rule-Based Classification Systems (FRBCSs). One of those functions is the CC-integral, where the product operations in the expanded form of the CI are generalized by copulas. Recently, some new Choquet-like operators were developed by generalizing the difference operation by a Restricted Dissimilarity Function (RDF) in either the usual or the expanded form of the original CI, also providing good results in practical applications. So, motivated by such developments, in this paper we propose the generalization of the CC-integral by means of RDFs, resulting in a function that we call d-CC-integral. We study some relevant properties of this new definition, focusing on its monotonicity-like behavior. Then, we proceed to apply d-CC-integrals in a classification problem, comparing different d-CC-integrals between them. The classification acuity of the best d-CC-integral surpasses the one achieved by the best CC-integral and is statistically equivalent to the state-of-the-art in FRBCSs.

descricao: Automated Feature Engineering (AutoFE) has become an important task for any machine learning project, as it can help improve model performance and gain more information for statistical analysis. However, most current approaches for AutoFE rely on manual feature creation or use methods that can generate a large number of features, which can be computationally intensive and lead to overfitting. To address these challenges, we propose a novel convolutional method called FeatGeNN that extracts and creates new features using correlation as a pooling function. Unlike traditional pooling functions like max-pooling, correlation-based pooling considers the linear relationship between the features in the data matrix, making it more suitable for tabular data. We evaluate our method on various benchmark datasets and demonstrate that FeatGeNN outperforms existing AutoFE approaches regarding model performance. Our results suggest that correlation-based pooling can be a promising alternative to max-pooling for AutoFE in tabular data applications.

descricao: Video summarization consists of generating a concise video representation that captures all its meaningful information. However, conventional summarization techniques often fall short of capturing all the significant events in a video due to their inability to incorporate the hierarchical structure of the video content. This work proposes an unsupervised method, named Hierarchical Time-aware Summarizer–HieTaSumm, that uses a hierarchical approach for that task. In this regard, hierarchical strategies for video summarization have emerged as a promising solution, in which video content is modeled as a graph to identify keyframes that represent the most relevant information. This approach enables the extraction of the frames that convey the central message of the video, resulting in a more effective and precise summary. Experimental results indicate that the proposed approach has great potential. Specifically, it seems to enhance coherence among different video segments, reducing frame redundancy in the generated summaries, and enhancing the diversity of selected keyframes.

descricao: College dropout is a concern for educational institutions since it directly impacts educational management and academic results, as well as being directly related to social problems. Therefore, there is significant incentive for studies that use data to support decisions by predicting risk of dropout so that institutions can attempt to prevent such cases. Although machine learning techniques were shown to have potential for this task, there are many steps involved when it comes to the use of real data, which comes from scattered systems and present issues such as need for data cleaning and preparation, high dimensionality of the data requiring adequate feature selection, as well as class imbalance. In this paper, we used data from 32.892 students enrolled between 2008 and 2020 from all courses offered by a public high-education institution. A protocol for data preparation is proposed and found to be more important than designing complex classifiers. We present guidelines when modelling a college dropout classification task using a public university data and experiments using Walk-Forward Validation that showed the predictive capacity for the first years.

descricao: People with autism spectrum disorder (ASD) may present, in addition to deficits in communication, social interaction and patterns of restricted and repetitive behaviors, also present a deficit in joint attention (JA), which refers to the response repertoire of following and/or directing an adult’s visual attention to objects or events in the environment. By having a strong relationship with the learning process, joint attention deficits can compromise a person’s learning process. In this way, the use of technology can help in the development of abilities in people with autism, such as, for example, improving joint attention, communication and social skills. In this context, the general objective of the work proposal was to develop a computational approach for intervention that allows the interaction of the student with autism, with 4 and 5 years old, with deficit in joint attention and social-communicative difficulties. Artificial intelligence (AI) techniques were used to model the most appropriate sequence and level of complexity of exercises for each child. AI resources were used with the intention of providing an intelligent environment to guide the child, dynamically and adaptively, in order to promote stimuli and adequate personalization of the process. In this way, it is intended to contribute significantly to the advancement of the state of the art regarding the production of computational technologies for people with ASD.

descricao: Education is the single most important investment that people can make in their futures, and since the Universal Declaration of Human Rights in 1948, the goal of achieving universal education has been on the international agenda. In this regard, there is no doubt that the Web has had a profound impact on making education both universally available and more relevant. Nevertheless, online courses are based on “static” learning material (one-size-fits-all). For that reason, it is not straightforward to assess learning with a great number of learners who differ considerably in their educational background, engagement styles, and cognitive skills. In this work, we aim to address these aforementioned challenges by proposing an explainable Machine Learning algorithm for personalizing web-based education systems. The method has a “deep” architecture mimicking the information representation structure in human brains, and it is continuously adapted based on the signals of the students, understanding their performance through micro-steps and maximizing the learning outcome. While our methodology is general and can be applied in numerous scenarios, we demonstrate its performance by a real case study which comprises a non-mandatory, standardized exam, that evaluates high school students in Brazil.

descricao: One common trend in recent studies of language models (LMs) is the use of standardized tests for evaluation. However, despite being the fifth most spoken language worldwide, few such evaluations have been conducted in Portuguese. This is mainly due to the lack of high-quality datasets available to the community for carrying out evaluations in Portuguese. To address this gap, we introduce the Brazilian Leading Universities Entrance eXams (BLUEX), a dataset of entrance exams from the two leading universities in Brazil: UNICAMP and USP. The dataset includes annotated metadata for evaluating the performance of NLP models on a variety of subjects. Furthermore, BLUEX includes a collection of recently administered exams that are unlikely to be included in the training data of many popular LMs as of 2023. The dataset is also annotated to indicate the position of images in each question, providing a valuable resource for advancing the state-of-the-art in multimodal language understanding and reasoning. We describe the creation and characteristics of BLUEX and establish a benchmark through experiments with state-of-the-art LMs, demonstrating its potential for advancing the state-of-the-art in natural language understanding and reasoning in Portuguese. The data and relevant code can be found at https://github.com/Portuguese-Benchmark-Datasets/BLUEX.

descricao: As agent-based systems have been growing, more and more the general public has access to them and is influenced by decisions taken by these systems. This increases the necessity for such systems to be capable of explaining themselves to a user. The Beliefs-Desires-Intentions (BDI) is a commonly used agent model that has an two-phase internal goal selection process (desires and intentions) to decide what goals to pursue. Belief-based goal processing (BBGP) model is an extended-BDI model whose selection process consists of four phases. This more-grained behavior may have relevant consequences for the analysis of what an intention is and better explain how an intention becomes what it is. Contrastive explanations are commonly employed by people and can bring benefits to the explanation exchange process. A Property-contrast (P-contrast) explanation is a type of contrastive explanation that compares the properties of an explanation object. Thus, we can take a goal as the object about which we want an explanation and its status in the selection process (that is, how much it has advanced) as a property. This work tackles the problem of generating P-contrast explanations and proposes a method to construct them. The method consists of two phases, which in turn consist of a set of steps. The first step of the second phase returns a set of beliefs that constitute a future explanation. Thus, this work focuses on the first phase and the first step of the second phase. We use a scenario of the cleaner world in order to illustrate the performance of our proposal.

descricao: Life in society requires constant communication and coordination. These abilities are efficiently achieved through sophisticated cognitive processes in which individuals are able to reason about the mental attitudes and actions of others. This ability is known as Theory of Mind. Inspired by human intelligence, the field of Artificial Intelligence aims to reproduce these sophisticated cognitive processes in intelligent software agents. In the field of multi-agent systems, intelligent agents are defined not only to execute reasoning cycles inspired by human reasoning but also to work similarly to human society, including aspects of communication, coordination, and organisation. Consequently, it is essential to explore the use of these sophisticated cognitive processes, such as Theory of Mind, in intelligent agents and multi-agent systems. In this paper, we conducted a literature review on how Theory of Mind has been applied to multi-agent systems, and summarise the contributions in this field.

descricao: Embedded artificial intelligence in IoT devices is presented as an option to reduce connectivity dependence, allowing decision-making directly at the edge computing layer. The Multi-agent Systems (MAS) embedded into IoT devices enables, in addition to the ability to perceive and act in the environment, new characteristics like pro-activity, deliberation, and collaboration capabilities to these devices. A few new frameworks and extensions enable the construction of agent-based IoT devices. However, no framework allows constructing them with hardware control, adaptability, and fault tolerance, besides agents’ communicability and mobility. This work presents an extension of the Jason framework for developing Embedded MAS with BDI agents capable of controlling hardware, communicating, and moving between IoT devices capable of dealing with fault tolerance. A case study of an IoT solution with a smart home, a monitoring center, and an autonomous vehicle is presented to demonstrate the framework’s applicability.

descricao: Regression models are commonly used to model the associations between a set of features and an observed outcome, for purposes such as prediction, finding associations, and determining causal relationships. However, interpreting the outputs of these models can be challenging, especially in complex models with many features and nonlinear interactions. Current methods for explaining regression models include simplification, visual, counterfactual, example-based, and attribute-based approaches. Furthermore, these methods often provide only a global or local explanation. In this paper, we propose a hybrid multilevel explanation (Hybrid Multilevel Explanation - HuMiE) method that enhances example-based explanations for regression models. In addition to a set of instances capable of representing the learned model, the HuMiE method provides a complete understanding of why an output is obtained by explaining the reasons in terms of attribute importance and expected values in similar instances. This approach also provides intermediate explanations between global and local explanations by grouping semantically similar instances during the explanation process. The proposed method offers a new possibility of understanding complex models and proved to be able to find examples statistically equal to or better than the main competing methods and to provide a coherent explanation with the context of the explained model.

descricao: The critical scenario in public health triggered by COVID-19 intensified the demand for predictive models to assist in the diagnosis and prognosis of patients affected by this disease. This work evaluates several machine learning classifiers to predict the risk of COVID-19 mortality based on information available at the time of admission. We also apply a visualization technique based on a state-of-the-art explainability approach which, combined with a dimensionality reduction technique, allows drawing insights into the relationship between the features taken into account by the classifiers in their predictions. Our experiments on two real datasets showed promising results, reaching a sensitivity of up to 84% and an AUROC of 92% (95% CI, [0.89–0.95]).

descricao: Acute Myeloid Leukemia (AML) is one of the most aggressive types of hematological neoplasm. To support the specialists’ decision about the appropriate therapy, patients with AML receive a prognostic of outcomes according to their cytogenetic and molecular characteristics, often divided into three risk categories: favorable, intermediate, and adverse. However, the current risk classification has known problems, such as the heterogeneity between patients of the same risk group and no clear definition of the intermediate risk category. Moreover, as most patients with AML receive an intermediate-risk classification, specialists often demand other tests and analyses, leading to delayed treatment and worsening of the patient’s clinical condition. This paper presents the data analysis and an explainable machine-learning model to support the decision about the most appropriate therapy protocol according to the patient’s survival prediction. In addition to the prediction model being explainable, the results obtained are promising and indicate that it is possible to use it to support the specialists’ decisions safely. Most importantly, the findings offered in this study have the potential to open new avenues of research toward better treatments and prognostic markers.

descricao: Federated Learning (FL) is a decentralized machine learning approach developed to ensure that training data remains on personal devices, preserving data privacy. However, the distributed nature of FL environments makes defense against malicious attacks a challenging task. This work proposes a new attack approach to poisoning labels using Bayesian neural networks in federated environments. The hypothesis is that a label poisoning attack model trained with the marginal likelihood loss can generate a less complex poisoned model, making it difficult to detect attacks. We present experimental results demonstrating the proposed approach’s effectiveness in generating poisoned models in federated environments. Additionally, we analyze the performance of various defense mechanisms against different attack proposals, evaluating accuracy, precision, recall, and F1-score. The results show that our proposed attack mechanism is harder to defend when we adopt existing defense mechanisms against label poisoning attacks in FL, showing a difference of 18.48% for accuracy compared to the approach without malicious clients.

descricao: Multiple aspect trajectory is a relevant concept that enables mining interesting patterns and behaviors of moving objects for different applications. This new way of looking at trajectories includes a semantic dimension, which presents the notion of aspects that are relevant facts of the real world that add more meaning to spatio-temporal data. Given the inherent complexity of this new type of data, the development of new data mining methods is needed. Despite some works have already focused on multiple aspect trajectory classification, few have focused on clustering. Although the literature presents several raw trajectory clustering algorithms, they do not deal with the heterogeneity of the semantic dimension. In this paper, we propose a novel hierarchical clustering algorithm for multiple aspect trajectories using a decision tree structure that chooses the best aspect to branch and group the most similar trajectories according to different criteria. We ran experiments using a well-known benchmark dataset extracted from a location-based social network and compared our clustering results with a state-of-the-art clustering approach over different internal and external validation metrics. As a result, we show that the proposed method outperformed the baseline, where it revealed a formation of more cohesive and homogeneous clusters in 88% of the clusters, being five times more precise according to the external metrics.

descricao: Carbonate reservoirs are known for their heterogeneity, which poses challenges in interpreting and defining geological models. The Brazilian reservoirs are formed mostly in highly faulted and fractured carbonate rocks, which can increase hydrocarbon transport and storage capacity. Strategies that permit the identification of these structures allow the optimization in the exploration of a reservoir. To fulfill this task, machine learning models have been able to provide an understanding of these environments through the use of data obtained by seismic methods. The use of convolutional neural networks (CNNs) has shown to be able to provide excellent abstractions in the field of semantic segmentation, including its use in seismic data. However, due to the highly heterogeneous formation of this type of data, the work of extracting information from these images remains challenging. From this, we investigate the potential of using Transformer models in this geological context focusing on the faults identification. As a technique to analyze this type of architecture, we use the TransUNet network, which combines the power of CNNs with the innovation brought by Transformers in a hybrid model of deep learning. To evaluate its performance, we made use of conventional CNNs to compare the results achieved. The results show that TransUNet outperforms conventional CNNs, achieving a Dice metric value of 88.34%, compared to 85.99% for U-Net, 83.41% for U-Net++, and 83.31% for SegNet, being also able to identify small structures beyond what is indicated in our target.

descricao: Pre-trained Transformer models have been used to improve the results of several NLP tasks, which includes the Legal Rhetorical Role Labeling (Legal RRL) one. This task assigns semantic functions, such as fact and argument, to sentences from judgment documents. Several Legal RRL works exploit pre-trained Transformers to encode sentences but only a few employ approaches other than fine-tuning to improve the performance of models. In this work, we implement three of such approaches and evaluate them over the same datasets to achieve a better perception of their impacts. In our experiments, approaches based on data augmentation and positional encoders do not provide performance gains to our models. Conversely, the models based on the DFCSC approach overcome the appropriate baselines, and they do remarkably well as the lowest and highest improvements respectively are 5.9% and 10.4%.

descricao: The demand for effective, efficient and safe methods for animal identification has been increasing significantly, due to the need for traceability, management, and control of this population, which grows at higher rates than the human population, particularly pets. Motivated by the efficacy of modern human identification methods based on face biometrics features, in this paper, we propose a dog face recognition method based on vision transformers, a deep learning approach that decomposes the input image into a sequence of patches and applies self-attention to these patches to capture spatial relationships between them. Results obtained on DogFaceNet, a public database of dog face images, show that the proposed method, which uses the EfficientFormer-L1 architecture, outperforms the state-of-the-art method proposed previously in literature based on ResNet, a deep convolutional neural network.

descricao: The ongoing COVID-19 pandemic caused an unprecedented overburning of healthcare systems and still represents a global health issue with the emergence of COVID-19 variants. The relevance of mass testing for COVID-19 in the find-test-trace-isolate-support strategy suggested by the World Health Organization (WHO) is imperative to reduce COVID-19 transmission. Although real-time polymerase chain reaction (RT-PCR) is considered a reference standard for COVID-19 detection, it is an expensive, lengthened, and laborious process, and problems in RNA extraction can reduce the sensitivity. In this context, the Raman spectroscopy analysis in biofluids is a label-free method performing a suitable cost-benefit application for COVID-19 detection. We propose a Convolutional Neural Network (CNN) architecture that processes spectra images generated by the Raman spectrum and returns the COVID-19 diagnosis of the spectrum sample. The predictive performance of the CNN was compared against several other algorithms widely adopted in the literature. The CNN architecture discriminates COVID-19 with Raman spectroscopy of blood samples with 96.8% accuracy, 95.5% sensitivity, and 98.2% of specificity, representing the best results as well as a promising alternative to distinguish samples. Moreover, we also present a model explanation analysis that contributes to clarifying the salient features taken into account by our CNN.

descricao: Graph-based image representation is a promising research direction that can capture the structure and semantics of images. However, existing methods for converting images to graphs often fail to preserve the hierarchical information of the image elements and produce sub-optimal or poor regions. To address these limitations, we propose a novel approach that uses a hierarchical image segmentation technique to generate graphs at multiple segmentation scales, capturing the hierarchical relationships between image elements. We also propose and train a Hierarchical Graph Convolutional Network for Image Classification (HGCIC) model that leverages the hierarchical information with three different adjacency setups on the CIFAR-10 database. Experimental results show that the proposed approach can achieve competitive or superior performance compared to other state-of-the-art methods while using smaller graphs.

descricao: Brain tumors pose a complex medical challenge, requiring a specific approach for accurate diagnosis and effective treatment. Early detection can significantly improve outcomes and quality of life for patients with brain tumors. Magnetic resonance (MRI) is a powerful diagnostic tool, and convolutional neural networks (CNNs) are efficient deep learning algorithms for image analysis. In this study, we explored using two CNN models for brain tumor classification and applied hyperparameter optimization and data augmentation techniques to achieve an accuracy of up to 96%. In addition, we use Explainable Artificial Intelligence (XAI) techniques to visualize and interpret the behavior of CNN models. Our results show that CNN models accurately classified MRI images with brain tumors. XAI techniques helped us to identify the patterns and features used by the models to make predictions. This study supports the development of more reliable medical diagnoses for brain tumors using CNN models and XAI techniques. The source code is available at (https://github.com/dieineb/Bracis23). The repository also has images generated during the experiments.

descricao: This study explores convolutional and recursive LSTM blocks within a singular architecture for forecasting stock prices. We propose a method that integrates convolutional networks, which learn to process signals through filters, with recursive LSTM blocks to account for critical temporal information often overlooked in convolutional approaches. Our investigation primarily revolves around two research questions: (1) Can integrating convolutional and recursive LSTM blocks within a singular architecture enhance prediction accuracy? and (2) What is the impact of training and testing with disparate data distributions? The latter question arises from our experiment of training the model using data drawn from the Indian Stock Market and testing the predictions with New York Stock Market data, thus deviating from the traditional focus on uniform stock market distributions. Our results reveal a notable improvement in prediction accuracy (MAPE reduction of 2.22%), strongly suggesting that pre-processing data via Convolutional Neural Networks (CNN) benefits LSTM blocks and can enhance the performance of stock market prediction methodologies.

descricao: The human gastrointestinal tract is prone to various abnormalities, including lethal diseases such as cancer, necessitating better endoscopic performance and standardized screening. Endoscopic scoring systems lack generalizability, emphasizing the need for artificial intelligence-based solutions. Using the HyperKvasir dataset, we employed deep learning, specifically Convolutional Neural Networks, or shortly CNNs, to analyze endoscopic images and videos. Our study focused on improving the classification of gastrointestinal tract diseases by proposing various CNN ensembles and fusion techniques. Through the use of seven CNN models and effective merging techniques, we achieved enhanced performance. Validation involved literature review and experiments. DenseNet-161 influenced the merger process, and integrating ResNet152 and VGG further enhanced effectiveness. Resource analysis included GPU model, RAM usage, and execution time. Results demonstrated comparable performance to the previous model, with F1-score of 0.910 and Matthews correlation coefficient, MCC for short, of 0.902, using 10 GB GPU RAM (compared to 15.8 GB). With 24.7 GB GPU RAM, F1-score of 0.913 and MCC of 0.905 were achieved. These findings advance our understanding of ensemble architectures and fusion techniques.

descricao: Over 470 million dogs are kept as pets around the world. Dogs are owned at an average number of 1.6% per household. The US has the most dog pets, where about 68% of households own at least one pet. Lost and missing dogs are a severe source of suffering and problems for their families. So, this paper addresses the problem of facial dog identification. This technology can benefit many applications, such as handling the missing pet problem, granting pets access to their houses, more intelligent zoonosis control, pet health care, and tracking stray pets. We evaluate a Residual Convolutional Neural Network, specifically ResNet-34, for facial identification in dogs. We tested in DogFaceNet and Flickr-dog datasets with and without two face preprocessing techniques: a central crop and an aligned facial extraction. Experimental results show promising results surpassing the state-of-the-art: 97.6% and 82.8% accuracies for DogFaceNet and Flickr-dog, respectively. Moreover, we also provide recall metrics for the best models.

descricao: Advances in textual classification can foster quality in existing clinical systems. Our research explored experimentally text classification methods applied in non-synthetic oncology clinical notes corpora. The experiments were performed in a dataset with 3,308 medical notes. Experiments evaluated the following machine learning and deep learning classification methods: Multilayer Perceptron Neural network, Logistic Regression, Decision Tree classifier, Random Forest classifier, K-nearest neighbors classifier, and Long-Short Term Memory. An experiment evaluated the influence of the corpora preprocessing step on the results, allowing us to identify that the classifier’s mean accuracy was leveraged from 26.1% to 86.7% with the per-clinical-event corpus and 93.9% with the per-patient corpus. The best-performing classifier was the Multilayer Perceptron, which achieved 93.90% accuracy, a Macro F1 score of 93.61%, and a Weighted F1 score of 93.99%.

descricao: Segmentation of Organs at Risk is a fundamental step during radiotherapy planning for cancer treatment. Its goal is to preserve healthy tissue around the tumor and ensure that the most radiation strikes only cancer cells. Physicians do this job manually, which can be slow and error-prone. Thus, automatic segmentation methodologies can speed up organ delimiting during radiotherapy planning. This work designs a method, EfficientDeepLab, a convolutional neural network architecture trained on CT scans for trachea segmentation, and obtained an 88.6% dice score.

descricao: Chest radiography exams are still one of the main methods for detecting and diagnosing certain thoracic pathologies. This study evaluates the performance of a DenseNet in a multi-label classification task on radiography images, using focal loss as the loss function to address the class imbalance problem. For the experiments, 14 different types of findings were considered. Satisfactory results were obtained using the area under the ROC curve (AUC-ROC) as the metric, where the average performance across all classes was 0.861.

descricao: Brain age prediction using neuroimaging data has shown great potential as an indicator of overall brain health and successful aging, as well as a disease biomarker. Deep learning models have been established as reliable and efficient brain age estimators, being trained to predict the chronological age of healthy subjects. In this paper, we investigate the impact of a pre-training step on deep learning models for brain age prediction. More precisely, instead of the common approach of pre-training on natural imaging classification, we propose pre-training the models on brain-related tasks, which led to state-of-the-art results in our experiments on ADNI data. Furthermore, we validate the resulting brain age biomarker on images of patients with mild cognitive impairment and Alzheimer’s disease. Interestingly, our results indicate that better-performing deep learning models in terms of brain age prediction on healthy patients do not result in more reliable biomarkers.

descricao: Swarm intelligence (SI) algorithms have become popular due to their self-learning characteristics and adaptability to external changes. They can find reasonable solutions to complex problems without in-depth knowledge. Much of the success of these algorithms comes from balancing the exploration and exploitation tasks. This work evaluates the application and performance of a reinforcement learning approach applied to a well-known swarm intelligence algorithm, Particle Swarm Optimization (PSO). We use the reinforcement learning agent Proximal Policy Optimization (PPO) to dynamically change the swarm communication topology according to the problem. We analyze the PSO’s behavior, influenced by the reinforcement learning agent, through methods such as interaction networks and fitness analysis. We show that the RL approach can transfer the knowledge learned from one function to other functions, and that dynamic changes of topology over time makes PSO much more efficient than setting only one specific topology, even when using a Dynamic topology. Our results then suggest that changing topologies might be more efficient than having a Dynamic topology, and that indeed Local and Global topologies have an important role in the best swarm performance. Our results take a step further on explaining the performance of SI and automatizing their use for non-experts.

descricao: The great complexity and size of electrical power systems makes their operation and control a challenging task. Maintaining a stable voltage profile to assure the security and stability of the system is one of many tasks that must be conducted daily by power system operators and its automatic control equipment. This work proposes a deep reinforcement learning framework for controlling the equipment responsible for keeping the voltages across the system buses within their limits. More specifically, a smart agent that is capable of deciding the best course of action in order to keep the system’s voltages within a specified range while taking into account system’s conditions is proposed. Besides the traditional deep reinforcement learning approach, three novel reinforcement learning variations named windowed, ensemble and windowed ensemble Q-Learning, which alter the agent’s learning process for voltage control, are presented and tested on IEEE 13, 37 and 123 bus systems, simulated on OpenDSS.

descricao: Computational face aging enables predicting a person’s future appearance using algorithms, with the goal that the output age is close to the expected age and that the individual’s characteristics are maintained. In this work, we evaluate the performance of four generative models on facial aging. Two models are based on generative adversarial networks (GANs), HRFAE, and SAM, and the other two are based on diffusion models, Pix2pix-zero and Instruct-pix2pix. The first two were explicitly trained to generate an aged version of the original person, and the others have a zero-shot generation; in other words, they are generic models that perform different tasks, including facial aging. Since diffusion models have been gaining attention because of their diversity and high-quality image generation, comparing their results against models specifically designed for the task using meaningful metrics is essential. Therefore, we compared these models using the FFHQ Aging database and with the metrics: Mean absolute error (MAE) of the predicted age, Fréchet inception distance (FID), and the cosine similarity of the FaceNet’s embeddings.

descricao: Face image de-occlusion and inpainting is a challenging problem in computer vision with several practical uses and is employed in many image preprocessing applications. The impressive results achieved by generative adversarial networks in image processing increased the attention of the scientific community in recent years around facial de-occlusion and inpainting. Recent network architecture developments are the two-stage networks using coarse to fine approach, landmarks, semantic segmentation map, and edge maps that guide the inpainting process. Moreover, improved convolutions enlarge the receptive field and filter the values passed to the next layer, and attention layers create relationships between local and distant information. This article presents a brief review of recent developments in GAN-based techniques for de-occlusion and inpainting of face images. In addition, it describes and analyzes network architectures and building blocks. Finally, we identify current limitations and propose directions for future research.

descricao: Since facial morphology can be linked to brain developmental problems, studies have been conducted to develop computational systems to assist in the diagnosis of some neurodevelopmental disorders based on facial images. The first steps usually include face detection and landmark identification. Although there are several libraries that implement different algorithms for these tasks, to the best of our knowledge no study has discussed the effect of choosing these ready-to-use implementations on the performance of the final classifier. This paper compares four libraries for facial detection and landmark identification in the context of classification of facial images for computer-aided diagnosis of Autism Spectrum Disorder, where the classifiers achieved 0.92, the highest F1-score. The results indicate that the choice of which facial detection and landmark identification algorithms to use do in fact affect the final classifier performance. It appears that the causes are related to not only the quality of face and landmark identification, but also to the success rate of face detection. This last issue is particularly important when the initial training sample size is modest, which is usually the case in terms of classification of some syndromes or neurodevelopmental disorders based on facial images.

descricao: Constructive Machine Learning (CML) is a research field that uses algorithms to generate new instances, similar but not identical to existing ones. It has been widely used to assist the discovery of new drug-like molecules. This is very challenging, given that the search space is discrete, unstructured and enormous. In this work we use CML to learn the intrinsic rules of datasets of molecules to generate novel ones. The chosen CML methods can be divided in two sub groups, text-based and graph oriented. Considering different possibilities to evaluate the methods and the generated molecules, we propose classifying generated molecules in a taxonomy, using a hierarchical multi-label classifier previously trained in a dataset of molecules with known taxonomy information. In this way, it is possible to predict properties and verify the relevance of the generated molecules to existing taxonomies. We also propose a hierarchical diversity measure to compare groups of molecules based on their taxonomy information. The measure showed coherent results and is faster to calculate than the commonly used external diversity measures.

descricao: Automated Machine Learning (AutoML) has achieved high popularity in recent years. However, most of these studies have investigated alternatives to single-label classification problems, presenting a need for more investigations in the multi-label classification scenario. From the AutoML point of view, the few studies on multi-label classification focus on automatically finding the best models based on mono-objective optimization. These tools train several multi-label classifiers in search of the one with the best performance in a single objective optimization process. In this work, we propose AutoMMLC, a new multi-objective AutoML method for multi-label classification, to find the best models that maximize the f-score measure and minimize the training time. Experiments were carried out with ten multi-label datasets and different versions of the proposed method using two multi-objective optimization algorithms: Multi-objective Random Search and Non-Dominated Sorting Genetic Algorithm II. We evaluated the Pareto front obtained by these methods through the hypervolume metric. The Wilcoxon test demonstrated that AutoMMLC versions had similar results for this metric. Multi-label Classification (MLC) algorithms were obtained from the Pareto frontiers through the Frugality Score and compared with the baseline algorithms. The Friedman test demonstrated that the MLC algorithms from AutoMMLC versions had equal performances to f-score and training time. Furthermore, they had better results than baseline algorithms for f-score and better results than most baseline algorithms for training time.

descricao: Accurately identifying plant species and varieties is crucial across various disciplines, such as biology, medicine, and agronomy. While species identification is challenging, variety identification presents an even greater difficulty. Conventional identification methods, although effective, often require specialized and costly equipment, making them less accessible. In this work, we explore the problem of hop variety classification, comparing traditional feature extraction methods with deep learning approaches using the UFOP-HVD dataset. We address two research questions: whether traditional techniques can achieve competitive results given the limited number of images and whether combining traditional techniques and deep learning can improve the current state-of-the-art. Our findings indicate that traditional techniques yield competitive results for hop variety identification, offering advantages such as interpretability, reduced computational costs, and potential integration into mobile devices. Moreover, we introduce an ensemble method that improves the accuracy from 77.16% to 81.90%, establishing a new state-of-the-art for the UFOP-HVD dataset. These results demonstrate the potential of merging traditional methods with deep learning for challenging hop variety classification tasks, providing an initial baseline for future research.

descricao: Quality classification of wood boards is an essential task in the sawmill industry, which is still usually performed by human operators in small to median companies in developing countries. Machine learning algorithms have been successfully employed to investigate the problem, offering a more affordable alternative compared to other solutions. However, such approaches usually present some drawbacks regarding the proper selection of their hyperparameters. Moreover, the models are susceptible to the features extracted from wood board images, which influence the induction of the model and, consequently, its generalization power. Therefore, in this paper, we investigate the problem of simultaneously tuning the hyperparameters of an artificial neural network (ANN) as well as selecting a subset of characteristics that better describes the wood board quality. Experiments were conducted over a private dataset composed of images obtained from a sawmill industry and described using different feature descriptors. The predictive performance of the model was compared against five baseline methods as well as a random search, performing either ANN hyperparameter tuning and feature selection. Experimental results suggest that hyperparameters should be adjusted according to the feature set, or the features should be selected considering the hyperparameter values. In summary, the best predictive performance, i.e., a balanced accuracy of 0.80, was achieved in two distinct scenarios: (i) performing only feature selection, and (ii) performing both tasks concomitantly. Thus, we suggest that at least one of the two approaches should be considered in the context of industrial applications.

descricao: When dealing with Deep Learning applications in open-set problems, accurately classifying known classes seen in the training phase is not the only aspect to be taken into account. In such a context, detecting Out-of-Distribution (OOD) samples plays an important role as an auxiliary task, generally solved by OOD detection methods. For medical applications, detecting unknown samples may in classification problems can be beneficial for many aspects, such as a better understanding of the diagnosis and probably a more adequate treatment. In this article, we evaluate a feature space-based approach, named as OpenPCS-Class, for OOD detection in medical applications, more specifically skin lesion classification. We compare the OpenPCS-Class against important OOD detection methods, evaluating different model architectures and OOD datasets. The OpenPCS-Class outperformed other methods at 48.4% and 5.3% in terms of FPR95 and AUROC, respectively.

descricao: The health domain has been largely benefited by Machine Learning solutions, which can be used for building predictive models to support medical decisions. But, for increasing the reliability of these systems, it is important to understand when the models are prone to failures. In this paper, we investigate what can we learn from the instances of a dataset which are hard to classify by Machine Learning models. Different reasons may explain why one or a set of instances are misclassified, despite the predictive model used. They can be either noisy, anomalous or placed in overlapping regions, to name a few. Our framework works at two levels: the original base dataset and a meta-dataset built to reflect the hardness level of the instances. A two-dimensional hardness embedding is assembled, which can be visually inspected to determine sets of instances to scrutinize better. We show some analysis that can be undertaken in this hardness space that allow to characterize why some of the instances are hard to classify, with case studies on health datasets.

descricao: The accurate identification of promoter regions in DNA sequences holds significant importance in the field of bioinformatics. While this problem has garnered substantial attention in the literature, it remains unresolved. Several researchers have achieved notable outcomes by employing diverse machine-learning techniques to predict promoter regions. However, only a few have thoroughly explored the utilization of features derived from the physicochemical properties of DNA across various organism types. This study investigates the advantages of incorporating these features in the training of machine-learning models. The research evaluates and compares the performance of multiple metrics on diverse datasets encompassing both prokaryotic and eukaryotic organisms. The state-of-the-art CNNProm method is employed as the baseline for our experiments. The models and source code associated with this study can be accessed at the following URL of the project’s repository: https://anonymous.4open.science/r/bracis-paper-1458/.

descricao: National Artificial Intelligence (AI) strategies have been implemented by several countries worldwide. These strategies aim to guide AI policy priorities and foster research, innovation, and development in AI. Alongside the development of national AI strategies, AI indices have emerged as tools to compare countries’ AI development levels. This study focuses on a specific AI indicator, the Global AI Index (GAII) proposed by Tortoise Media, which ranks 62 countries based on their level of investment, innovation, and implementation of AI. The GAII computes a ranking of countries by aggregating sub-dimensions within these categories using a weighted sum approach, with subjective weight assignments. This paper critically analyzes the weighting and aggregation approaches used in the GAII, employing two techniques. Firstly, the Stochastic Multicriteria Acceptability Analysis (SMAA) is used to explore changes in rankings by varying the weights. Secondly, a non-additive aggregation model known as the Choquet integral is applied to consider potential interactions among dimensions. The findings indicate that the weights assigned to criteria strongly influence the final ranking. Additionally, there are interactions between AI dimensions that should be taken into account to address unbalanced achievements across dimensions. This study contributes to the development of more robust and objective methodologies for comparing countries’ AI development levels.

descricao: The running time complexity is a crucial measure for determining the computational efficiency of a given program or algorithm. Depending on the problem complexity class, it can be considered intractable; a program that solves this problem will consume so many resources for a sufficiently large input that it will be unfeasible to execute it. Due to Alan Turing’s halting problem, it is impossible to write a program capable of determining the execution time of a given program and, therefore, classifying it according to its complexity class. Despite this limitation, an approximate running time value can be helpful to support development teams in evaluating the efficiency of their produced code. Furthermore, software-integrated development environments (IDEs) could show real-time efficiency indicators for their programmers. Recent research efforts have made, through artificial intelligence techniques, complexity estimations based on code characteristics (e.g., number of nested loops and number of conditional tests). However, there are no databases that relate code characteristics with complexity classes considered inefficient (e.g., and O(n!)), which limits current research results. This research compared three machine learning approaches (i.e., Random Forest, eXtreme Gradient Boosting, and Artificial Neural Networks) regarding their accuracy in predicting Java program codes’ efficiency and complexity class. We train each model using a dataset that merges data from literature and 394 program codes with their respective complexity classes crawled from a publicly available website. Results show that Random Forest resulted in the best accuracy, being 90.17% accurate when predicting codes’ efficiency and 89.84% in estimating complexity classes.

descricao: Statistical tests of hypothesis play a crucial role in evaluating the performance of machine learning (ML) models and selecting the best model among a set of candidates. However, their effectiveness in selecting models over larger periods of time remains unclear. This study aims to investigate the impact of statistical tests on ML model selection in sequential experiments. Specifically, we examine whether selecting models based on statistical tests leads to higher quality models after a significant number of iterations and explore the effect of the number of tests performed and the preferred statistical test for different experimental time horizons. The study on binary classification problems reveals that the use of statistical tests should be approached with caution, particularly in challenging scenarios where generating improved models is difficult. The analysis demonstrates that statistical tests may impede progress and impose overly stringent acceptance criteria for new models, hindering the selection of high-quality models. The findings also indicate that the dominance of versions without statistical tests remained consistent, suggesting the need for further research in this area. Although this study is limited by the number of datasets and the absence of pre-test assumption verification, it emphasizes the importance of understanding the impact of statistical tests on ML model selection.

descricao: There is great interest in the creation of genetically modified organisms that use amino acids different from the naturally encoded amino acids. Unnatural amino acids have been incorporated into genetically modified organisms to develop new drugs, fuels and chemicals. When incorporating new amino acids, it is necessary to change the standard genetic code. Expanded genetic codes have been created without considering the robustness of the code. In this work, multi-objective genetic algorithms are proposed for the optimization of expanded genetic codes. Two different approaches are compared: weighted and Pareto. The expanded codes are optimized in relation to the frequency of replaced codons and two measures based on robustness (for polar requirement and molecular volume). The experiments indicate that multi-objective approaches allow to obtain a list of expanded genetic codes optimized according to combinations of the three objectives. Thus, specialists can choose an optimized solution according to their needs.

descricao: The MAX-CUT Problem involves dividing a set of n vertices in a weighted graph G = (V, E) into two subsets (S, S) in such a way that the sum of the weights between the subsets is maximized. This research introduces two heuristic methods that combine Genetic Algorithm, Tabu Search, and a set of optimality cuts, which are also proven in this work. To the best of our knowledge, we are the first to utilize these inequalities in conjunction with the genetic algorithm methodology to solve the MAX-CUT problem. Computational experiments using a benchmark set of 54 instances, ranging from 800 to 3000 vertices, demonstrate that the incorporation of optimality cuts is a crucial factor for our methodologies to compete effectively with six state-of-the-art approaches for the MAX-CUT problem and our genetic algorithm that incorporated optimality cuts in the population generation was able to improve the state-of-the-art value for the G51 instance and find the same solutions as the literature in 31 other instances.

descricao: Robust optimization considers uncertainty in the decision variables while noisy optimization concerns with uncertainty in the evaluation of objective and constraint functions. Although many evolutionary algorithms have been proposed to deal with robust or noisy optimization problems, the research question approached here is whether these methods can deal with both types of uncertainties at the same time. In order to answer this question, we extend a test function generator available in the literature for multi-objective optimization to incorporate uncertainties in the decision variables and in the objective functions. It allows the creation of scalable and customizable problems for any number of objectives. Three evolutionary algorithms specifically designed for robust or noisy optimization were selected: RNSGA-II and RMOEA/D, which utilize Monte Carlo sampling, and the C-RMOEA/D, which is a coevolutionary MOEA/D that uses a deterministic robustness measure. We did experiments with these algorithms on multi-objective problems with (i) uncertainty in the decision variables, (ii) noise in the output, and (iii) with both robust and noisy problems. The results show that these algorithms are not able to deal with simultaneous uncertainties (noise and perturbation). Therefore, there is a need for designing algorithms to deal with simultaneously robust and noisy environments.

descricao: Bio-inspired optimization algorithms aim to address the most diverse problems without the need for derivatives, and they are independent of the shape of the search space. The Flying Squirrel Optimizer belongs to the family of bio-inspired algorithms and simulates the movement of flying squirrels from tree to tree in search of food. This paper proposes a binary version of the flying squirrel optimizer for feature selection problems. To elucidate the performance of the proposed algorithm, we employed six other well-known bio-inspired algorithms for comparison purposes in sixteen benchmark datasets widely known in the literature. Furthermore, we employ the binary flying squirrel optimizer in selecting gas concentrations to identify faults in power transformers. The results expressed that Binary Flying Squirrell Optimizer can either find compact feature sets or improve classification effectiveness, corroborating its robustness.

descricao: AutoML addresses the challenge of automatically configuring machine learning pipelines for specific data analysis tasks. These pipelines encompass techniques for preprocessing and classifying data. Numerous approaches exist for discovering the optimal pipeline configuration, with most focusing on optimization methods such as Bayesian optimization and evolutionary algorithms. Nevertheless, limited knowledge exists regarding the structure of the search space that these methods operate within. What is certain is that these spaces incorporate categorical, continuous, and conditional hyperparameters, and effectively handling them is not straightforward. To shed light on this matter, the present study conducts an examination of AutoML search spaces generated by the Tree-based Pipeline Optimization Tool (TPOT) algorithm utilizing local optimal networks (LON). The goal is to gain deeper insights into the overall characteristics of the search space, enhancing comprehension of the search strategy employed and the algorithm’s limitations. This investigation aids in understanding the search strategy and constraints of the algorithm, ultimately contributing to the advancement of novel optimization algorithms or the refinement of existing ones within the scientific literature. The findings have implications for enhancing optimization algorithms by illuminating how the search space is explored and the consequent impact on the discovered solutions.

descricao: Transfer learning through language modeling achieved state-of-the-art results for several natural language processing tasks such as named entity recognition, question answering, and sentiment analysis. However, despite these advancements, some tasks still need more specific solutions. This paper explores different approaches to enhance the performance of Named Entity Recognition (NER) in transformer-based models that have been pre-trained for language modeling. We investigate model soups and domain adaptation methods for Portuguese language entity recognition, providing valuable insights into the effectiveness of these methods in NER performance and contributing to the development of more accurate models. We also evaluate NER performance in few/zero-shot learning settings with a causal language model. In particular, we evaluate diverse BERT-based models trained on different datasets considering general and specific domains. Our results show significant improvements when considering model soup techniques and in-domain pretraining compared to within-task pretraining.

descricao: Semi-supervised learning is characterized by a low number of labeled instances and a high number of unlabeled instances. FlexCon-C (Flexible Confidence Classifier) is a well-known semi-supervised method that uses the self-training learning algorithm as basis to generate prediction models. The main difference between self-training and FlexCon-C is that the former uses a fixed threshold to select the unlabeled instances, while the latter has a dynamically adjusted confidence. FlexCon-C applies a confidence adjustment equation based on the classifier performance. In this sense, the classifier performance is used to select and to label unlabeled instances. In Machine Learning, it is well-known that the classifier performance can be further improved through the use of classifier ensembles. Therefore, this study proposes the use classifier ensembles in the FlexCon-C confidence adjustment equation, aiming to provide a more efficient measure to select and to label unlabeled instances. In order to assess the viability of the proposed method (FlexCon-CE), an empirical analysis will be conducted, using 20 datasets, three different classification algorithms and five different configurations of initially unlabeled data. The results indicate that the proposed method outperformed the traditional method, therewith proving itself promising for the task of automatic data selection and labeling in the semi-supervised context.

descricao: Single image super-resolution (SISR) consists of obtaining one high-resolution version of a low-resolution image by increasing the number of pixels per unit area. This method has been actively investigated by the research community, due to the wide variety of problems ranging from real-world surveillance to aerial and satellite imaging. Most of the improvements in SISR come from convolutional networks, in which approaches often focus on the deeper and wider architectural paradigm. In this work, we decided to step up from the traditional convolutions and adopt the concept of capsules. Since their overwhelming results in image classification and segmentation problems, we question how suitable they are for SISR. We also verify that different solutions share similar configurations, and argue that this trend leads to fewer explorations of network designs. Throughout our experiments, we check various strategies to improve results, ranging from new and different loss functions to changes in the capsule layers. Our network achieved positive and promising results with fewer convolutional-based layers, showing that capsules might be a concept worth applying to the image super-resolution problem. In particular, we observe that the proposed method recreates the connection between the different characters more precisely, thus demonstrating the potential of capsules in super-resolution problems.

descricao: Dengue is a disease that is endemic to certain regions, and in 2022, it was responsible for more than three million cases in the Americas. One of the most effective ways to prevent Dengue is by preventing the formation of breeding sites for the Aedes aegypti mosquito, which is the primary vector of the disease. Unfortunately, identifying these breeding sites remains a challenge as citizens lack knowledge to distinguish Ae. Aegypti larvae from other species. A solution can be the development of a Deep Learning model, to be deployed in a mobile application that classifies mosquito species using photos of larvae. Currently only a few models are available that mostly differentiate only between genera (Aedes versus non-Aedes), or present very low accuracy. Therefore, the objective of this research is to develop an image classification model that can differentiate between Ae. Aegypti, Aedes albopictus, and Culex sp. Larvae using pictures taken with a cellphone camera by comparing various Deep Learning models (Mobilenetv2, ResNet18, ResNet34, EfficientNet_B0 and EfficientNet_Lite0). Best results were obtained with EfficientNet_Lite0 with an accuracy of 97.5% during validation and 90% during testing, an acceptable result considering the risks related to a misclassification in this context. These results demonstrate the viability of a classification of mosquito larvae differentiating even between Aedes species and thus providing a contribution to the prevention of dengue.

descricao: The leaf is the organ of the plant body that performs photosynthesis and its area is one of the morphological parameters that most respond to droughts, climate changes, and attack of pathogens, associated with the accumulation of biomass and agricultural productivity. In addition, leaf area and other surface data (for example, width and length) are widely used in studies of plant anatomy and physiology. The methods of measuring these leaf surface parameters are often complicated and costly. In this context, this work aims to develop a simple and low-cost method capable of accurately measuring the leaf surface size of plant species with significant agricultural interest. Our method extract the information through images of leaves accompanied by a scale pattern whose real area is known, captured by a simple camera. To evaluate our method, we performed experiments with images of 118 leaves of 6 species. We compared the results to the ImageJ software, which is widely used to estimate leaf dimensions from images. The results showed our method present performance similar to ImageJ. However, unlike ImageJ, our method does not require user interaction during the dimensions estimation.

descricao: Autonomous robots for agricultural tasks have been researched to great extent in the past years as they could result in a great improvement of field efficiency. Navigating an open crop field still is a great challenge; RTK-GNSS is a excellent tool to track the robot’s position, but it needs precise mapping and planning while also being expensive and signal dependent. As such, onboard systems that can sense the field directly to guide the robot are a good alternative. Those systems detect the rows with adequate image techniques and estimate the position by applying algorithms to the obtained mask, such as the Hough transform or linear regression. In this paper, a direct approach is presented by training a neural network model to obtain the position of crop lines directly from an RGB image. While, usually, the camera in such systems are looking down to the field, a camera near the ground is proposed to take advantage of tunnels formed between rows. A simulation environment for evaluating both the model’s performance and camera placement was developed and made available in Github, and two datasets to train the models are proposed. The results are shown across different resolutions and stages of plant growth, indicating the system’s capabilities and limitations.

descricao: Artificial intelligence approaches, such as computer vision, can help better understand the behavior of bees and management. However, the accurate detection and tracking of bee species in the field remain challenging for traditional methods. In this study, we compared YOLOv7 and YOLOv8, two state-of-the-art object detection models, aiming to detect and classify Jataí Brazilian native bees using a custom dataset. Also, we integrated two tracking algorithms (Tracking based on Euclidean distance and ByteTrack) with YOLOv8, yielding a mean average precision (mAP50) of 0.969 and mAP50–95 of 0.682. Additionally, we introduced an optical flow algorithm to monitor beehive entries and exits. We evaluated our approach by comparing it to human performance benchmarks for the same task with and without the aid of technology. Our findings highlight occlusions and outliers (anomalies) as the primary sources of errors in the system. We must consider a coupling of both systems in practical applications because ByteTrack counts bees with an average relative error of 11%, EuclidianTrack monitors incoming bees with 9% (21% if there are outliers), both monitor bees that leave, ByteTrack with 18% if there are outliers, and EuclidianTrack with 33% otherwise. In this way, it is possible to reduce errors of human origin.

descricao: Eye gaze trackers are devices designed to identify an individual’s gaze fixation in relation to a screen or another reference point. These tools are widely applied in usability testing as they provide various metrics for studying how people interact with applications. In the past, these tools were expensive and required a controlled environment, as well as trained personnel for proper operation. Although nowadays, new implementations do not require physical hardware to perform these tests, they often rely on license-based models instead of being open source. The objective of this work is to create a standalone system that enables any user to implement a low-cost eye gaze tracker using web technologies. The goal is to facilitate its use in remote and face-to-face studies in a simple way, requiring only a computer and a webcam. We evaluated the impact of three different calibration techniques on the performance of a regression-based prediction algorithm in eye-tracking. In our experiments, the best result of linear regression was obtained with a circular calibration system that uses 160 points. Furthermore, we integrated the system with a web interface and an API, enabling users to record their usability sessions and analyze fixation points through heatmaps.

descricao: In which we investigate the technical issues surrounding the defeat, or perhaps the sudden assassination, of the Winograd Schema Challenge. We argue that, while the obvious suspect is the WinoGrande-based solution, the real cause of death was the masked language modeling technique for learning large language models. The Winograd Schema Challenge was, in the end, just a test for masked language closure, and as such it was killed by the use of this technique at scale.

descricao: As the capabilities of language models continue to advance, it is conceivable that “one-size-fits-all” model will remain as the main paradigm. For instance, given the vast number of languages worldwide, many of which are low-resource, the prevalent practice is to pretrain a single model on multiple languages. In this paper, we add to the growing body of evidence that challenges this practice, demonstrating that monolingual pretraining on the target language significantly improves models already extensively trained on diverse corpora. More specifically, we further pretrain GPT-J and LLaMA models on Portuguese texts using 3% or less of their original pretraining budget. Few-shot evaluations on Poeta, a suite of 14 Portuguese datasets, reveal that our models outperform English-centric and multilingual counterparts by a significant margin. Our best model, Sabiá-65B, performs on par with GPT-3.5-turbo. By evaluating on datasets originally conceived in the target language as well as translated ones, we study the impact of language-specific pretraining in terms of 1) capturing linguistic nuances and structures inherent to the target language, and 2) enriching the model’s knowledge about a domain or culture. Our results indicate that most benefits stem from the domain-specific knowledge acquired through monolingual pretraining. Finally, we show that our optimized model for Portuguese demonstrates a reduced performance in English tasks, thereby substantiating the inherent compromise in refining models for specific linguistic domains.

descricao: This paper explores methods to disambiguate Part-of-Speech (PoS) tags for closed class words in Brazilian Portuguese corpora annotated according to the Universal Dependencies annotation model. We evaluate disambiguation methods of different paradigms, namely a Markov-based method, a widely adopted parsing tool, and a BERT-based language modeling method. We compare their performances with two baselines, and observe a significant increase of more than 10% over the baselines for all proposed methods. We also show that while the BERT-based model outperforms the others reaching for the best case a 98% accuracy predicting the correct PoS tag, the use of the three methods as an Ensemble method offers more stable result according to the smaller variance for the numerical results we performed.

descricao: The biomedical NLP community has seen great advances in dataset development mostly for the English language, which has hindered progress in the field, as other languages are still underrepresented. This study introduces a dataset of Brazilian Portuguese annotated for named entity recognition and relation extraction in the healthcare domain. We compiled and annotated a corpus of health professionals’ responses to frequently asked questions in online healthcare forums on diabetes. We measured inter-annotator agreement and conducted initial experiments using up-to-date methods to recognize entities and extract relations, such as BERT-based ones. Data, models, and results are publicly available at https://github.com/pavalucas/Bete.

descricao: Language models trained with Bidirectional Encoder Representations from Transformers (BERT) have demonstrated remarkable results in various Natural Language Processing (NLP) tasks. However, the legal domain poses specific challenges for NLP due to its highly specialized language, which includes technical vocabulary, formal style, frequent use of law citations and semantics based on vast knowledge. Therefore, pretrained language models on a generic corpus may not be suitable for performing specific legal domain tasks. They lack the necessary expertise to understand the nuances of legal language, leading to inaccuracies and inconsistencies. This work describes the development of a specialized language model, LegalBert-pt, for the legal domain in Portuguese. The model was pretrained on a large and diverse corpus of Brazilian legal texts and is now open-source and customizable for specific tasks. Experiments were conducted to evaluate the pretrained model’s effectiveness in the legal domain, both intrinsically and in two specific tasks: named-entity recognition and text classification. The results indicate that using LegalBert-pt outperforms the generic language model in all tasks, emphasizing the importance of specialization in achieving effective results for specific tasks in the legal domain.

descricao: Social networks have become the main stage for discussion on various current topics. In particular, electoral processes tend to bring many publications with polarized opinions on political issues addressed by candidates. A comprehensive analysis of social media publications on high-impact controversial topics and the opinions expressed in them could contribute to a clearer understanding of the dynamics of political discussion, providing valuable insights for society. In this context, we investigate how to apply a clustering-based topic modeling approach to produce public evaluation information on different current issues, in particular controversial political topics. We propose a framework that enriches text representations, combining state-of-the-art unsupervised (HDBSCAN) and supervised (BERTimbau) techniques to identify controversial political topics in social media publications in Brazilian Portuguese. To this end, weekly collections were carried out on the social network Twitter, making it possible to identify controversial events for each analyzed date. We compare the controversial topics uncovered with real-world news to validate the results and compare our method with a traditional method described in the literature.

descricao: Advances in sign language processing have not adequately kept pace with the tremendous progress that has been made in oral language processing. This fact serves as motivation for conducting research on the potential utilization of deep learning models within the domain of sign language processing. In this paper, we present a method that utilizes deep learning to build a latent and generalizable representation space for signs, leveraging Formal SignWriting notation and the concept of sentence-based representation to effectively address sign language tasks, such as sign classification. Extensive experiments demonstrate the potential of this method, achieving an average accuracy of 81% on a subset of 70 signs with only 889 training data and 69% on a subset of 338 signs with 3, 871 training data.

descricao: In clustering problems where the objective is not based on specifically spatial proximity, but rather on feature patterns and the semantic description, traditional internal cluster validation indices might not be appropriate. This article proposes a novel validity index to suggest the most appropriate number of clusters based on a semantic description of categorical databases. To assess our index, we also propose a synthetic data generator specifically designed for this type of application. We tested data sets with different configurations to assess the performance of the proposed index compared to well-known indices in the literature. Thus, we demonstrate that the index has great potential for discovering the number of clusters for the type of application studied and the data generator is able to produce relevant data sets for the internal validation process.

descricao: Epidemics start within a network because of the existence of epidemic sources that spread information over time to other nodes. Data about the exact contagion pattern among nodes is often not available, besides a simple snapshot characterizing nodes as infected, or not. Thus, a fundamental problem in network epidemic is identifying the set of source nodes after the epidemic has reached a significant fraction of the network. This work tackles the multiple source detection problem by using graph neural network model to classify nodes as being the source of the epidemic. The input to the model (node attributes) are novel epidemic information in the k-hop neighborhoods of the nodes. The proposed framework is trained and evaluated under different network models and real networks and different scenarios, and results indicate different trade-offs. In a direct comparison with prior works, the proposed framework outperformed them in all scenarios available for comparison.

descricao: MicroRNAs (miRNAs) are crucial regulators of gene expression, including in diseases such as cancer. Although machine learning methods have shown promise in predicting miRNA-target interactions, they encounter challenges related to imbalanced classes and false positives. To tackle these issues, this study proposes a GNN-based model, using a variant of GraphSAGE algorithm named HinSAGE, which integrates validated miRNA-mRNA and mRNA-mRNA interactions with cancer-related gene expression data. Results show that our approach effectively learns miRNA-target interaction patterns from the graph structure and node features. The model achieves 77% precision, 80% recall, 78% F1-score, and 86% ROC AUC on the test data. It competes well with related approaches, reaching an F1-score of approximately 90% on a common test set. Thus, GNNs offer a promising avenue for studying miRNA-target interactions, providing balanced predictive power and improved precision through negative interaction sampling from the graph.

descricao: In this study, we examine the impact of human mobility on the transmission of COVID-19, a highly contagious disease that has rapidly spread worldwide. To investigate this, we construct a mobility network that captures movement patterns between Brazilian cities and integrate it with time series data of COVID-19 infection records. Our approach considers the interplay between people’s movements and the spread of the virus. We employ two neural networks based on Graph Convolutional Network (GCN), which leverage spatial and temporal data inputs, to predict time series at each city while accounting for the influence of neighboring cities. In comparison, we evaluate LSTM and Prophet models that do not capture time series dependencies. By utilizing RMSE (Root Mean Square Error), we quantify the discrepancy between the actual number of COVID-19 cases and the predicted number of cases by the model among the models. Prophet achieves the best average RMSE of 482.95 with a minimum of 1.49, while LSTM performs the least despite having a low minimum RMSE. The GCRN and GCLSTM models exhibit mean RMSE error values of 3059.5 and 3583.88, respectively, with the lowest standard deviation values for RMSE errors at 500.39 and 452.59. Although the Prophet model demonstrates superior performance, its maximum RMSE value of 52,058.21 is ten times higher than the highest value observed in the Graph Convolutional Networks (GCNs) models. Based on our findings, we conclude that GCNs models yield more stable results compared to the evaluated models.

descricao: Domestic violence has increased globally as the COVID-19 pandemic combines with economic and social stresses. Some works have used traditional feature extractors to identify features from sound signals to detect physical violence. However, these extractors have not performed well at recognizing physical violence in audio. Besides, the use of Machine Learning is limited by the trade-off between collecting more data while keeping users privacy. Federated Learning (FL) is a technique that allows the creation of client-server networks, in which anonymized training result can be uploaded to a central model, responsible for aggregating and keeping the model up to date, and then distribute the updated model to the client nodes. In this paper, we proposed a FL approach to the violence detection problem in audio signals. The framework was evaluated on a newly proposed synthetic dataset, in which audio signals are represented as mel-spectrograms images, augmented with violence extracts. Thereby, it treats it as a problem of image classification using pre-trained Convolutional Neural Networks (CNN). Inception v3, MobileNet v2, ResNet152 v2 and VGG-16 architectures were evaluated, with the MobileNet architecture presenting the best performance, in terms of accuracy (71.9%), with a loss of 3.6% when compared to the non-FL setting.

descricao: Several crimes occur daily, and the initial investigation begins with a police report. In cities with high crime rates, it is impractical to expect the police to read and analyze every crime narrative. Some police reports may involve multiple victims or the same crime may be reported more than once. Additionally, police reports may exhibit similarities due to a shared modus operandi. This study addresses the challenge of providing a police report and searching for the most similar report in the database. A similar police report can be either another report with overlapping words or one that shares a similar modus operandi. One potential solution is to represent each police report as a feature vector and compare these vectors using a similarity function. Different methods can be employed to represent the narrative, including embedding vectors and count-based approaches such as TF-IDF. This research explores the use of pre-trained embedding representations at both the word and sentence levels, such as Universal Sentence Encoder, Word2Vec, RoBERTa, Doc2Vec, among others. We determine the most effective representation for capturing semantic and lexical similarities between police reports by comparing different embedding models. Furthermore, we compare the effectiveness of available pre-trained embedding models with a model trained specifically on a corpus of police reports. Another contribution of this work is the development of trained embedding models specifically tailored for the domain of police reports.

descricao: Public procurement plays a crucial role in government operations by acquiring goods and services through competitive bidding processes. However, the increasing volume of procurement data has made manual analysis impractical and time-consuming. Therefore, text clustering and topic modeling techniques have been widely used to uncover hidden patterns in unstructured text data. This paper leverages the power of BERT-based models to overcome the challenges associated with analyzing public procurement data. Specifically, we employ BERTopic, a topic modeling technique based on BERT, to generate clusters that capture the underlying topics in procurement data. Additionally, we evaluate several sentence embedding models for representing procurement documents. By combining BERT-based models and advanced sentence embeddings, we aim to enhance the accuracy and interpretability of topic modeling in public procurement analysis. Our results provide valuable insights into the underlying topics within the data, aiding decision-making processes and improving the efficiency of procurement operations.

descricao: Energy consumption reduction is an increasing trend in machine learning given its relevance in socio-ecological importance. Consequently, it is important to quantify how real-time learning algorithms tailored for data streams and edge computing behave in terms of accuracy, processing time, memory usage, and energy consumption. In this work, we bring forward a tool for measuring energy consumption in the Massive Online Analysis (MOA). First, we analyze the energy consumption rates obtained by our tool against a gold-standard hardware solution, thus showing the robustness of our approach. Next, we experimentally analyze classification algorithms under different validation protocols and concept drift and highlight how such classifiers behave under such conditions. Results show that our tools enable the identification of different classifiers’ energy consumption. In particular, it allows a better understanding of how energy consumption rates vary in drifting and non-drifting scenarios. Finally, given the insights obtained during experimentation on existing classifiers, we make our tool publicly available to the scientific community so that energy consumption is also accounted for in developing and comparing data stream mining algorithms.

descricao: The production of energy from renewable sources has become a more sustainable and environmentally correct alternative, where the use of solar energy through photovoltaic systems is evident. One of the main problems in the use of photovoltaic systems is the high cost of installation and maintenance of this system, in addition to the cost of the residential electricity tariff, which makes this technology expensive for most residential consumers in Brazil. An alternative for consumers to get around the high amounts paid on the energy bill is to opt for the white tariff modality, which is characterized by offering the variation of the energy value according to the day and time of consumption. The present work aims to develop a fuzzy system to manage the energy production from a photovoltaic system, optimizing the use of the produced energy between the consumer, the battery and the electric grid in a white tariff scenario for residential units in Brazil. Based on the simulations, the fuzzy system presented is efficient, with a significant economic reduction in the energy bill compared to a simple photovoltaic system without the ability to make intelligent decisions and used commercially in industries.

descricao: Using technologies capable of providing retina structure high-resolution images is one of the most widespread means of identifying structural changes that may indicate the onset or progression of visual impairment. Automated glaucoma detection using optical coherence tomography is still considered an area needing further research. Several manual analyzes are currently performed over the generated by imaging equipment. This work presents an approach to foster automatic glaucoma evaluation considering convolutional neural networks for semantic segmentation of retinal layers through optical coherence tomography images and image processing for measuring the cup region in the optic nerve head portion. We provide a quantitative evaluation comparing the results obtained by a specialist physician. The work’s main contribution consists of the first approach supporting the automation of a new biomarker for diagnosing glaucoma.

descricao: Active Self-Learning algorithms reduce the labeled data required to train a Machine Learning model through supervised training. This paper explores various Active Self-Learning algorithms for named entity recognition tasks. Firstly, we investigate the impact of different self-training techniques on Active Self-Learning algorithms. Secondly, we propose a novel token-level Active Self-Learning algorithm that achieves near-peak performance using fewer hand-annotated tokens compared to existing works. Through numerous experiments, we found that the sentence-level Active Self-Learning algorithm did not consistently yield significant results compared to pure active learning. However, our proposed token-level Active Self-Learning algorithm showed promising performance, training a neural model to nearly peak accuracy with fewer human-annotated tokens compared to state-of-the-art active learning baseline algorithms. The experimental results are presented and discussed, demonstrating the superior performance of the token-level Active Self-Learning algorithm